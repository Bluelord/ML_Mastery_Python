{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Boston House Prices\n\nRegression predictive modeling machine learning problem from end-to-end Python","metadata":{}},{"cell_type":"markdown","source":"Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970.<br>\nThe attributes are deﬁned as follows (taken from the UCI Machine Learning Repository1): <br>\n\nThe data dictionary for this dataset is as follows:<br>\n\n| Feature Name | Deatail | Keys |\n|--------------|---------|------|\n| CRIM | per capita crime rate by town.| Continuous |\n| ZN | proportion of residential land zoned for lots over 25,000 sq.ft. | Continuous |\n| INDUS | proportion of non-retail business acres per town. | Continuous |\n| CHAS | Charles River dummy variable | 1 = tract bounds river; 0 = otherwise |\n| NOX | nitric oxides concentration (parts per 10 million) | Continuous |\n| RM | average number of rooms per dwelling | Int |\n| AGE | proportion of owner-occupied units built prior to 1940 | Continuous |\n| DIS | weighted distances to ﬁve Boston employment centers | Continuous |\n| RAD | index of accessibility to radial highways | Int |\n| TAX | full-value property-tax rate per 10,000 | Continious |\n| PTRATIO | pupil-teacher ratio by town | Continuous |\n| B | 1000(Bk−0.63)2 where Bk is the proportion of blacks by town | Continous |\n| LSTAT | % lower status of the population | Continuous |\n| MEDV | Median value of owner-occupied homes in 1000s | Continuous |","metadata":{}},{"cell_type":"code","source":"# Load libraries\n%matplotlib inline  \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read CSV files into DataFrame","metadata":{}},{"cell_type":"code","source":"filename = '/kaggle/input/boston-house-prices/housing.csv'\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n'B', 'LSTAT', 'MEDV']\nBoston = pd.read_csv(filename, delim_whitespace=True, names=names)\nBoston.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for missing values in train data\ntotal_nan = Boston.isna().sum().sort_values(ascending=False)\npercent_nan = (Boston.isna().sum() / Boston.isnull().count()).sort_values(ascending=False)\nmissing_values = pd.concat([total_nan, percent_nan], axis=1, keys=['Total', 'Percent'])\nmissing_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"Boston.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature = np.reshape(names, (7,2))\nfig, axes = plt.subplots(7, 2, figsize=(10, 15))\nfor i in range(7):\n    for j in range(2):\n        sns.histplot(data = Boston, x = feature[i][j], bins=25 , kde=True, ax = axes[i,j])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = Boston.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(15, 10))\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True, cmap='coolwarm', annot=True, fmt=\".1f\",annot_kws={'size':16}, ax = ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data = Boston)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a lot of structure in this dataset. We need to think about transforms that we could use\nlater to better expose the structure which in turn may improve modeling accuracy. So far it\nwould be worth trying:<br>\n- Feature selection and removing the most correlated attributes.\n- Normalizing the dataset to reduce the e\u000bect of di\u000bering scales.\n- Standardizing the dataset to reduce the e\u000bects of di\u000bering distributions.","metadata":{}},{"cell_type":"code","source":"# Split-out validation dataset\narray = Boston.values\nX = array[:,0:13]\ny = array[:,13]\nseed = 70\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test options and evaluation metric\nnum_folds = 10\nseed = 70\nscoring = 'neg_mean_squared_error'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spot-Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare Algorithms\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',\nLinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO',\nLasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN',\nElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\nKNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',\nDecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare Algorithms\nfig = plt.figure(figsize = (8,5))\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN Algorithm tuning\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nk_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(n_neighbors=k_values)\nmodel = KNeighborsRegressor()\nkfold = KFold(n_splits=num_folds)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensembles\nensembles = []\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB',\nAdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM',\nGradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF',\nRandomForestRegressor(n_estimators=10))])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET',\nExtraTreesRegressor(n_estimators=10))])))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare Algorithms\nfig = plt.figure(figsize =  (8,5))\nfig.suptitle('Scaled Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tune scaled GBM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=np.array([50,100,150,200,250,300,350,400]))\nmodel = GradientBoostingRegressor(random_state=seed)\nkfold = KFold(n_splits=num_folds)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=seed, n_estimators=400)\nmodel.fit(rescaledX, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform the validation dataset\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(Y_validation, predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}