{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_Feature Selection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKOgsU6C+yv8WJ273FjP1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluelord/ML_Mastery_Python/blob/main/06_Feature_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NyJDM2Tzwee"
      },
      "source": [
        "# Feature Selction\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2CdCROW0p2h",
        "outputId": "f169d014-971b-42a0-dc5a-125fac202e27"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1LER4ctz4nm"
      },
      "source": [
        "Features in the dataset are very important for our model perfomance accuracy, it can also decrese performance of many ML models, especially for linear ML model. Feture selction is a crusial step befor modeling your ML model, the advantages of selecting relavent features will **Reduce Overfitting, Improves Accuracy, Reduces Traning Time**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY5MN6dF1bcm"
      },
      "source": [
        "## Univariable Selection\n",
        "\n",
        "Statistical test can be done  on the data to find the stringest relation with the out, **SelectKBest** class use to select the best feature from the test, in  this example **Chi-Square** test is used non-negative features.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG45WN39yNQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a4ebb2-e0e3-430e-ed3a-b8c0ba8d1797"
      },
      "source": [
        "# Feature Extraction with univariable Statictical test (Chi-Squaare for classification)\n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "# Load Data\n",
        "filename = \"/content/drive/MyDrive/Colab Notebooks/ML Mastery python/Dataset/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas','pres','skin','test','mass','pedi','age','class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "# separate array into input & output \n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# Feature extraction\n",
        "##########################################\n",
        "test = SelectKBest(score_func=chi2, k = 4) # we can change the score fun & k is the no of feature we want to use\n",
        "fit = test.fit(X, Y)\n",
        "###########################################\n",
        "#Summarize scores \n",
        "set_printoptions(precision=3)\n",
        "print(fit.scores_)\n",
        "features = fit.transform(X)\n",
        "# Summarizing the selected features\n",
        "print(features[:,:])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
            "[[148.    0.   33.6  50. ]\n",
            " [ 85.    0.   26.6  31. ]\n",
            " [183.    0.   23.3  32. ]\n",
            " ...\n",
            " [121.  112.   26.2  30. ]\n",
            " [126.    0.   30.1  47. ]\n",
            " [ 93.    0.   30.4  23. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPFnejGpODxj"
      },
      "source": [
        "## Recursive Features Elimination\n",
        "\n",
        "RFE uses the model accuracy to identify which features are important by recursivly removing the features and building new model dataset again and again. **RFE** class from sklearn is used for this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrdLOvNpNzKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9ebd7f-457e-4b3b-8223-2258f4990aca"
      },
      "source": [
        "# Feature extraction wtih RFE\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Load Data\n",
        "# Feature extraction\n",
        "##########################################\n",
        "model = LogisticRegression(solver='liblinear') # In this we need a model for recurrently select the feature and test the dataset.\n",
        "rfe = RFE(model, 3)\n",
        "fit = rfe.fit(X,Y)\n",
        "###########################################\n",
        "# Summarizing the selected features\n",
        "print(\"Num Features: %d:\" % fit.n_features_)\n",
        "print(\"Selected Features:\" % fit.support_)\n",
        "print(\"Feature Ranking:\" % fit.ranking_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Features: 3:\n",
            "Selected Features:\n",
            "Feature Ranking:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44s9kmuGeHoJ"
      },
      "source": [
        "## Principal Component Analysis\n",
        "\n",
        "PCA is one if the data comprasion technique, it choose the number of features or the principal components and result into new dataset. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMUiCETmdRc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e50d9a-302d-4685-b28e-90228ee95055"
      },
      "source": [
        "# Feature Extraction with PCA \n",
        "from sklearn.decomposition import PCA\n",
        "# Load Data\n",
        "\n",
        "# Feature extraction\n",
        "##########################################\n",
        "pca = PCA(n_components= 3)\n",
        "fit = pca.fit(X)\n",
        "# Summarize components\n",
        "print(\"Explained Varience: %s\" % fit.explained_variance_ratio_)\n",
        "print(fit.components_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained Varience: [0.889 0.062 0.026]\n",
            "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
            "   5.372e-04 -3.565e-03]\n",
            " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
            "  -8.168e-04 -1.402e-01]\n",
            " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
            "  -6.400e-04 -1.255e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01RLpSaDg1Iw"
      },
      "source": [
        "## Feature Importance \n",
        "\n",
        "Decision trees like Random forest & Extra Trees can be used to estimate the importance of Features. Score given to the features by this class higlight the importance of the features in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWQnW74Ygi2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afeb1422-6cb9-4a58-982c-5c08949b4273"
      },
      "source": [
        "# Feature Extraction with Extra Trees Classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# Load Data\n",
        "\n",
        "# Feature extraction\n",
        "model = ExtraTreesClassifier(n_estimators=100)\n",
        "model.fit(X,Y)\n",
        "print(model.feature_importances_)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.112 0.236 0.099 0.078 0.077 0.141 0.119 0.138]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
