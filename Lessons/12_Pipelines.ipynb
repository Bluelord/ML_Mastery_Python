{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_Pipelines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPa1oaj2FmKXItNP0I/Epcz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluelord/ML_Mastery_Python/blob/main/12_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcdpzOzoot9V"
      },
      "source": [
        "# Automated ML workflows with Pipelines\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCF4X-7b3WMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f48c3e1-3fa8-4ff6-e4d0-c816a65b7008"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-tD9I2up8mt"
      },
      "source": [
        "Pipelines is used to automate the ML workflow, it is use to minimize the data leakage, constructing modeled pipeline for data preparation and feature extraction. \n",
        "There are some standared workflows in pipelines, it work by allowing linear squence of data transformed to chained together culminating in a modeling process that can be evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUaZt1YUrMBM"
      },
      "source": [
        "## Data Preparation and Modelling Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQTgVMIh1CCA"
      },
      "source": [
        "Some of the Trap you may face is leaking of data from training set to test set, to avoid  this we need to strongly saperate the training & testing dataset. For Example Preparing the entire training dataset before learing would not be valid step becaouse it may influence by scale in testset.\n",
        "Pipelining will prevent this data leakage by ensuring that hte data preparation is constained to each fold of your cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6FOuJDyuTLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba8ae2d-7b92-4650-8962-dbd7d02e96e3"
      },
      "source": [
        "# Camparing a pipeline that standardizes the data then creates a model\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import FeatureUnion \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Load Data\n",
        "filename = \"/content/drive/MyDrive/Colab Notebooks/ML Mastery python/Dataset/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas','pres','skin','test','mass','pedi','age','class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "# separate array into input & output \n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# Create Pipeline \n",
        "estimators = []\n",
        "estimators.append(('standadize', StandardScaler()))\n",
        "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
        "model = Pipeline(estimators)\n",
        "\n",
        "# Evaluating the Pipeline\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(model, X, Y, cv= kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.773462064251538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWHmshD2rcF6"
      },
      "source": [
        "## Feature Extraction & Modeling Pipeline\n",
        "\n",
        "This also lead to data leakage, it must also restructed to your training data. The tool called **FeatureUnion** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSHLVY2_uTeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fb8ac2-06c0-4b1a-fdbf-a2b6c67d771b"
      },
      "source": [
        "# Camparing a pipeline that standardizes the data then creates a model\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "# Load Data\n",
        "filename = \"/content/drive/MyDrive/Colab Notebooks/ML Mastery python/Dataset/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas','pres','skin','test','mass','pedi','age','class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "# separate array into input & output \n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# create feature union\n",
        "features = []\n",
        "features.append(('pca', PCA(n_components=3)))\n",
        "features.append(('select_best', SelectKBest(k=6)))\n",
        "feature_union = FeatureUnion(features)\n",
        "# create pipeline\n",
        "estimators = []\n",
        "estimators.append(('feature_union', feature_union))\n",
        "estimators.append(('logistic', LogisticRegression(solver='liblinear')))\n",
        "model = Pipeline(estimators)\n",
        "# evaluate pipeline\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7760423786739576\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
