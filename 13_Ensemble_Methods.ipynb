{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_Ensemble Methods.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVAr9vUWxE9XDiixFTQ0fe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluelord/ML_Mastery_Python/blob/main/13_Ensemble_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOaJkeicuhtV"
      },
      "source": [
        "## Improving Performance\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ysykloq3i0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916f8c31-2d4b-4099-9cea-199e2e294756"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5VO04e6rihs"
      },
      "source": [
        "*   Bagging: Buildig Multiple models (typicaplly of same type) from different subsamples of the triaing dataset.\n",
        "*   Boosting: Building Multiple Models (typically of same type) each of which learn to fix the prediciton error of prior model in the sequence.\n",
        "*   Voting: Building multiple model (typically of different type) & simple statictics (like calculating the mean) are used to combine the prediction.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlJqim3D-jlu"
      },
      "source": [
        "## Bagging Algorithms\n",
        "\n",
        "Bagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning.\n",
        "\n",
        "Random Forests is an extension of bagged decision trees. Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers.\n",
        "\n",
        "Extra Trees are another modi\fcation of bagging where random trees are constructed from samples of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnYBWr5Wus6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddfd9802-2b9b-41b5-c65d-23dc5978161e"
      },
      "source": [
        "# Bagged Dicision Tree\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load Data\n",
        "filename = \"/content/drive/MyDrive/Colab Notebooks/ML Mastery python/Dataset/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas','pres','skin','test','mass','pedi','age','class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "# separate array into input & output \n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "seed = 7\n",
        "kfold = KFold(n_splits=10)\n",
        "cart = DecisionTreeClassifier()\n",
        "trees = 100\n",
        "model = BaggingClassifier(base_estimator = cart, n_estimators = trees, random_state=seed)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.770745044429255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2jbqSYEsICb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6fd44a-7aad-437c-bd85-4518efce6b38"
      },
      "source": [
        "# Random Forest classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "##############################################\n",
        "seed = 7\n",
        "kfold = KFold(n_splits=10)\n",
        "trees = 100\n",
        "model = RandomForestClassifier(n_estimators = trees, max_features = 3)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7734107997265892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrBtEImbsHv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6351ce41-609c-4d28-dd7f-b0e03512a330"
      },
      "source": [
        "# Extra tree classification\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "##############################################\n",
        "seed = 7\n",
        "kfold = KFold(n_splits=10)\n",
        "trees = 100\n",
        "model = ExtraTreesClassifier(n_estimators = trees, max_features = 3)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7577238550922762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab8TrsWBA6wk"
      },
      "source": [
        "## Boosting Algorithms\n",
        "\n",
        "AdaBoost generally works by weighting instances in the dataset by how easy or di\u000ecult they are to classify, allowing the algorithm to pay less or more attention to them in the construction of subsequent models.\n",
        "\n",
        "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf1fJ9RyutYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5374c14-7051-49f4-b98a-cd7d63b9b45a"
      },
      "source": [
        "# AdaBoost Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "filename = \"/content/drive/MyDrive/Colab Notebooks/ML Mastery python/Dataset/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_trees = 30\n",
        "seed=7\n",
        "kfold = KFold(n_splits=10)\n",
        "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.760457963089542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCs-EXWGsIii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0942e038-0b23-4ca9-8c81-32a6a4425601"
      },
      "source": [
        "# Stochastic Gradient Boosting Classification\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "#######################################\n",
        "seed = 7\n",
        "num_trees = 100\n",
        "kfold = KFold(n_splits=10)\n",
        "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7681989063568012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H7COl-Frlbq"
      },
      "source": [
        "## Voting Ensemble\n",
        "\n",
        "Voting is one way of combining the predictions from multiple ML algorithms. It works by creating two or more standalone models from your training dataset.\n",
        "A Voting Classifier can then be used to wrap your models and average the predictions of sub-models when asked to make predictions for new data. The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually is difficult. More advanced methods can learn how to best weight the predictions from sub-models, but this is called stacking (stacked aggregation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSrKehFKut5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7275b015-aeac-4d3e-9d1b-8b78e9ba54c9"
      },
      "source": [
        "# Voting Ensemble for Classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "filename = \"/content/drive/MyDrive/Colab Notebooks/ML Mastery python/Dataset/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "kfold = KFold(n_splits=10)\n",
        "# create the sub models\n",
        "estimators = []\n",
        "model1 = LogisticRegression(max_iter=10000)\n",
        "estimators.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "estimators.append(('cart', model2))\n",
        "model3 = SVC(max_iter=10000)\n",
        "estimators.append(('svm', model3))\n",
        "# create the ensemble model\n",
        "ensemble = VotingClassifier(estimators)\n",
        "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7643028024606973\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}